{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Central Limit Theorem\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability and Statistics - Recap\n",
    "\n",
    "* Random experiments are experiments whose outcome cannot be predicted with certainty.\n",
    "    * Tossing a coin\n",
    "* Sample space is the set of all possible outcomes of a random experiment.\n",
    "    * S = {H, T}\n",
    "* An outcome is just an element of the sample space.\n",
    "    * H is the outcome corresponding to getting a heads on a coin toss\n",
    "* An event is a subset of the sample space.\n",
    "    * A = {H} Event A occurs when you get a heads on a single coin toss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "---\n",
    "\n",
    "* Random variable is a function that maps the sample space to R (real numbers).\n",
    "    * x : S --> R (w is used to indicate an event)\n",
    "    \n",
    "$$\n",
    "x =  \\left\\{ \\begin{array}{lr} 1, & \\text{if } w = \\{H\\}\\\\ 0, & \\text{if } w = \\{T\\}\\\\ \\end{array}\\right\\}\n",
    "$$\n",
    "\n",
    "* The cumulative distribution function (c.d.f) is a function F: R --> [0,1] defined by $F(x) = P(X <= x)$ where X is the random variable and x is an arbitary real number.\n",
    "    * $X$ ~ $F$ indicates F is the distribution of r.v. X\n",
    "\n",
    "$$\n",
    "P(a < X <= b) = P(X <= b) - P(X <= a) = F(b) - F(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X < a) = F(a^-)\n",
    "$$\n",
    "\n",
    "#### 1. Discrete r.v.\n",
    "\n",
    "* Discrete random variable - A random variable is said to be disrete if the set of all possible values of X is a sequence x_1, x_2 (i.e. countable finite or infinite sequence) \n",
    "    \n",
    "* Probability mass function (p.m.f) p: R --> [0,1] such that $p(x) = P(X = x)$.\n",
    "\n",
    "$$\n",
    "\\sum_{i} p(x_i) = 1\n",
    "$$\n",
    "\n",
    "* Relation between c.d.f and p.m.f\n",
    "\n",
    "$$\n",
    "F(a) = \\sum_{x_i <= a} p(x_i)\n",
    "$$\n",
    "\n",
    "#### 2. Continuous r.v.\n",
    "\n",
    "* Continuous random variable - A random variable is said to be continuous if there exists a non-negative function f(x) defined for all x $\\in$ R such that for any set B of rea numbers, we have $P(X \\in B) = \\int_B f(x) dx$.\n",
    "    \n",
    "* Probability density function (p.d.f) is the above defined f(x).\n",
    "\n",
    "$$\n",
    "\\int_R f(x) dx = 1\n",
    "$$\n",
    "\n",
    "* Relation between c.d.f and p.d.f\n",
    "\n",
    "$$\n",
    "F(a) = \\int_{-\\infty}^{a} f(x) dx\n",
    "$$\n",
    "\n",
    "#### 3. Expectation\n",
    "\n",
    "* Expectation is the average or mean value of a random variable.\n",
    "* For discrete random variable,\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{x} x.p(x)\n",
    "$$\n",
    "\n",
    "* For continuous random variable,\n",
    "\n",
    "$$\n",
    "E[X] = \\int_{R} x.f(x) dx\n",
    "$$\n",
    "\n",
    "* Expectation of sum of random variables is equal to the sum of expectation of random variables, $\\textbf {irrespective of mutual independence}$, i.e.\n",
    "\n",
    "$$\n",
    "E[\\sum_{i=1}^{n}X_i] = \\sum_{i=1}^{n}E[X_i]\n",
    "$$\n",
    "\n",
    "* $E[c] = c$ where c is a constant\n",
    "* $E[cX] = cE[X]$ where c is a constant\n",
    "\n",
    "#### 4. Variance\n",
    "\n",
    "* Variance of X is defined by $Var(X) = E[(x-\\mu)^2]$\n",
    "* Variance of sum of random variables is equal to the sum of expectation of random variables $\\textbf {only if all the random variables are mutually independent}$ i.e.\n",
    "\n",
    "$$\n",
    "Var(\\sum_{i=1}^{n}X_i) = \\sum_{i=1}^{n}Var(X_i)\n",
    "$$\n",
    "\n",
    "* $Var(c) = 0$\n",
    "* $Var(X + c) = Var(X)$ where c is a constant\n",
    "* $Var(cX) = c^{2}Var(X)$ where c is a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common distributions\n",
    "---\n",
    "Consider X as the random variable.\n",
    "\n",
    "````{tabbed} Bernoulli\n",
    "* In Bernoulli distribution, r.v. can take only two possible values 0 or 1 with X=1 occuring with probability p.\n",
    "\n",
    "|    Distribution   |   Type     |   p.m.f      | E[X] | Var(X) |\n",
    "| :------------     |  -------------        |  -------------        |----:| ----:  |\n",
    "| Bernoulli         |Discrete |    $p(x) =  \\left\\{ \\begin{array}{lr} 1-p, & \\text{if x = 0}\\\\ p, & \\text{if x = 1}\\\\ \\end{array}\\right\\} $  |  $p$ | $p(1-p)$ |\n",
    "\n",
    "```{image} ../assets/2022_01_10_central_limit_theorem_notebook/bernoulli-pmf.png\n",
    ":name: Bernoulli p.m.f.\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "````{tabbed} Binomial\n",
    "\n",
    "* In n Binomial events, we can get the probability of getting r events $(r\\le n)$ with favourable odds p and the rest with unfavourable odds q $(q = 1-p)$.\n",
    "\n",
    "|    Distribution   |   Type     |   p.m.f    | E[X] | Var(X) |\n",
    "| :------------     |  -------------        |  -------------        |----:| ----:  |\n",
    "| Binomial          |Discrete |$ p(r) = {n \\choose r }p^{r}q^{n-r}$|$np$|$np(1-p)$|\n",
    "\n",
    "```{image} ../assets/2022_01_10_central_limit_theorem_notebook/binomial-pmf.png\n",
    ":name: Binomial p.m.f.\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Gaussian\n",
    "\n",
    "* The normal/gaussian distribution can be expressed in terms of its expectation value and variance.\n",
    "\n",
    "|    Distribution   |   Type     |   p.d.f      | E[X] | Var(X) |\n",
    "| :------------     |  -------------        |  -------------        |----:| ----:  |\n",
    "| Normal/Gaussian   |Continuous |  $ f(x) = \\frac{1}{σ\\sqrt{2Π}}\\exp{(-\\frac{1}{2}{(\\frac{x-\\mu}{σ})}^2)}$|$μ$|$σ$|\n",
    "\n",
    "```{image} ../assets/2022_01_10_central_limit_theorem_notebook/gaussian-pdf.png\n",
    ":name: Gaussian p.d.f.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition for CLT\n",
    "---\n",
    "\n",
    "#### 1. Probability distributions\n",
    "\n",
    "Let's say we want to explore the statistical properties of the heights of human adults. Let us divide the range of possible adult human heights, say from 4 feet to 7 feet, into bins of size 1 feet. We measure the height of a random person and put it in the corresponding bin; like a measurement of 5.6 feet goes into the bin [5 feet,6 feet). When we take multiple such measurements and stack them on top on each other in the respective bins, we get a histogram. This histogram tells us about the distribution of height among the people we measured. If we decrease our bin size and increase the number of measurements, we can make statements and estimates about the distribution more precisely. \n",
    "\n",
    "However, it may happen one of the bin may remain empty as no measurment that we sampled fell in the range of that bin. However, that does not mean that no sample in the entire population will fall in that range. So, we use a curve to approximate the historgram and provide us with the same information. The advantage of a curve is that we can account for the empty bins and we do not need to take care of the bin size while sampling.\n",
    "\n",
    "We dont have enough money and time to sample the entire population. The curve approximated based on the mean and standard deviation of the data we collect is a good enough estimate for the entire population.\n",
    "\n",
    "```{admonition} Takeaway\n",
    "The probability distribution (as the name suggests) shows us how the probability of measurements is distributed.\n",
    "```\n",
    "\n",
    "We plot a histogram of the height data obtained from the [Kaggle weight-height dataset](https://www.kaggle.com/mustafaali96/weight-height) and try to estimate a curve that fits the data.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../weight-height.csv\")\n",
    "arr = list(df['Height'])\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mu, std = norm.fit(arr)\n",
    "\n",
    "# Plot the histogram.\n",
    "plt.hist(arr, bins=25, density=True)\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = '''Heights histogram\n",
    "''' + \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Height x (inches)\")\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```{image} ../assets/2022_01_10_central_limit_theorem_notebook/prob-dist.png\n",
    ":name: Probability distribution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sampling a distribution\n",
    "\n",
    "If we ask a computer to pick a measurment at random based on the probability described by the historgram/curve, it is likely to fall in the taller region of the histogram/curve. Every once in a while the computer will also pick some measurment from the shorter region of the histogram/curve. \n",
    "\n",
    "```{admonition} Takeaway\n",
    "Sampling a distribution (again as the name suggests) is getting samples from a distribution based on the probabilities described by the distribution.\n",
    "```\n",
    "\n",
    "Why do we have to sample from a distribution? We can use the computer to generate a lot of samples and we can plug them into various statistical tests and make inferences about the real-world data. Since we know the original distribution of the data we collect, we can compare our expectations of what will happen to what the reality is. So, sampling allows us to determine what statistical tests are capable of doing with doing much work in data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normal or Gaussian distribution\n",
    "\n",
    "The normal or Gaussian distribution is a bell-shaped, symmetric curve. In context of our adult human heights example, the x-axis represents the heights and the y-axis represents the probability of observing someone of that height.\n",
    "\n",
    "Properties\n",
    "- The normal distributions are always centred at the average value.\n",
    "- A tall, narrow curve means less range of the quantity on the x-axis\n",
    "- The width of the curve is captured by the standard deviation $\\sigma$\n",
    "- Almost 68% of measurments fall with $\\pm 1 \\sigma$\n",
    "- Almost 95% of the measurements fall with $\\pm 2 \\sigma$\n",
    "- Almost 99.7% of the measurements fall with $\\pm 3 \\sigma$\n",
    "- Given an average value and the standard deviation, we can draw a unique normal distribution.\n",
    "- We observe a curve a lot in nature, such as for distributions of heights, weights, commute times, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "---\n",
    "\n",
    "#### Even if you are not normal, the average is normal.\n",
    "\n",
    "Let us take a uniform distribution in the range $x \\in [0, 1]$. The probability of picking any number in this range is uniform. We take 30 points at random from this range and call it sample 1. We calculate the mean of this sample and plot a histogram of the mean. We collect multiple such samples and plot their means on the histogram. When we take many such means, we see that the histogram represents the normal/Gaussian curve. Even when we have sampled the data from a uniform distribution, the means of the sampling distributions are normally distributed. Even when we begin with an exponential distribution, the means of the sampling distribution still end up being normally distributed. \n",
    "\n",
    "Practical implications of the means being nornally distributed\n",
    "- In reality, we don not know what distribution our data belongs to. However, the CLT does not care about the original distribution of our data. So we do not have to worry about the distribution that the samples come from. \n",
    "\n",
    "CLT is the basis for a lot of statistics. The normally distributed means are used to make confidence intervals and in various other statistical tests that help differentiate between two or more samples.\n",
    "\n",
    "```{caution}\n",
    "It is believed that the CLT is applicable only when we take a sample size $\\ge$ 30.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Statement of CLT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbf5712d8d7b097a8202ab85b07b61e1f30061650a94f21811f295bf10edc2d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
